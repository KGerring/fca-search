from retrieval.search_engine import SearchEngine
from other.data import getStopWords
from other.constants import DATABASES_FOLDER, DATA_FOLDER, SETTINGS_FILE
from retrieval.index import Index
from fca_extension.fca_search_engine import FCASearchEngine
import getopt, sys
from common.io import readfile
from preprocess.index_manager import IndexManager
from other.stopwatch import Stopwatch
from other.settings import Settings
import json
from other.interface import *
from common.czech_stemmer import createStem

## functions 
def printSearch(searchResults, stopwatch):
	results = searchResults['origin']
	documents = results['documents']
	
	if documents:
		maxURL = max(len(x['url']) for x in documents) - len('http://')
		maxKeywords = max([len(repr([(x[0], round(x[1])) for x in y['keywords'][:3]])) for y in documents])
		maxPos = len(str(len(documents)))
		maxScore = max(len(str(round(x['score'], 3))) for x in documents) + 1
		
		print(''.ljust(maxPos), 'URL'.ljust(maxURL), 'KEYWORDS'.ljust(maxKeywords), 'SCORE'.ljust(maxScore), 'WORDS')
		for pos, item in enumerate(documents):
			score = str(round(item['score'], 3))
			url = item['url'].replace('http://', '')
			keywords = repr([(x[0], round(x[1])) for x in item['keywords'][:3]])
			wordscount = item['words']
	
			print(str(pos+1).ljust(maxPos), url.ljust(maxURL), keywords.ljust(maxKeywords), score.ljust(maxScore), wordscount)
	
	gen, sib, spec, meta, sugg = getFcaExt(searchResults)
	if stopwatch:
		meta['time'] = str(round(stopwatch.total, 4))
	
	print()
	print('Generalization' + str(gen))
	print('Specialization:' + str(spec))
	print('Siblings:' + str(sib))
	print('Search query: ' + str(results['parsedQuery']))
	print('Number results: ' + str(len(documents)))
	print()
	print('Did you mean: {0}'.format(sugg))
	print('Meta: {0}'.format(meta))

	
def printJson(searchResults, stopwatch):
	gen, sib, spec, meta, spellcheck = getFcaExt(searchResults)
	
	results = searchResults['origin']
	documents = results['documents']
	fca = {'gen':gen, 'spec':spec, 'sib':sib}
	if stopwatch:
		meta['time'] = stopwatch.total
	data = {'documents':documents, 'fca': fca, 'meta' : meta, 'spellcheck':spellcheck}
	jsonData = json.dumps(data)
	print(jsonData)
	
def getFcaExt(searchResults):
	spec = searchResults['specialization']
	gen = searchResults['generalization']
	sib = searchResults['siblings']
	meta = searchResults['meta']
	spellcheck = searchResults['suggestions']
	
	return gen, sib, spec, meta, spellcheck


def search(databaseName, query, outFormat, stopwatch):
	searchResults = searchQuery(databaseName, query, stopwatch)
	if stopwatch:
		stopwatch.elapsed('Done')
	if outFormat == 'console':
		printSearch(searchResults, stopwatch)
	elif outFormat == 'json':
		printJson(searchResults, stopwatch)

	
def rebuildIndex(databaseName, newLinks = []):
	database = DATABASES_FOLDER + databaseName + '/'
	settings = Settings(database + SETTINGS_FILE)
	indexManager = IndexManager(settings)
	indexManager.shutUp = False
	indexManager.rebuild(newLinks, database, getStopWords())
	
	
## main

if __name__ == '__main__':
	stopwatch = Stopwatch().start()
	longOptions = ['keylen', 'keywordsCount', 'keyScoreLimit', 'charset',
					'minKeywords', 'dynamicKeywords', 'maxKeywords', 
					'disallowExtensions', 'maxDocumentsInContext', 'maxKeywordsPerDocument',
					'stem', 'frequency', 'docID', 'findURL', 'findDocID', 'docFrequency']

	longEqOptions = [x + '=' for x in longOptions]
	longDashOptions = ["--" + x for x in longOptions]

	parameters = ['links', 'linkscount', 'words']
	dashParameters = ['--' + x for x in parameters]

	try:
		opts, args = getopt.getopt(sys.argv[1:], 'd:q:b:p:f:m:r:a:t', longEqOptions + parameters)
		opts = dict(opts)
		currSettings = {k[2:]:v for k,v in opts.items() if k in longDashOptions}
		
		# search in database
		if '-q' in opts:
			outFormat = opts.get('-f', 'console')
			search(opts['-d'], opts['-q'], outFormat, stopwatch if '-t' not in opts else None)
				
		# build index
		elif '-b' in opts:
			databaseName = opts['-b']
			linksSourcePath = opts.get('-p', DATA_FOLDER + opts['-b'] + '.txt')
			maxLinksToDownload = int(opts.get('-m', 0))
			buildIndex(databaseName, linksSourcePath, currSettings)
		
		# refresh index
		elif '-r' in opts:
			rebuildIndex(opts['-r'])
			
		# add links to index
		elif '-a' in opts:
			databaseName = opts['-a']
			linksSourcePath = opts.get('-p')
			links = readfile(linksSourcePath).splitlines()
			rebuildIndex(databaseName, links)

		# stemmer
		elif '--stem' in opts:
			print(createStem(opts['--stem']))

		# API call
		elif '-d' in opts:
			databaseName = opts['-d']
			if '--links' in opts:
				print(getAllLinks(databaseName))
			elif '--linkscount' in opts:
				print(len(getAllLinks(databaseName)))
			elif '--words' in opts:
				print(getAllWords(databaseName))
			elif '--frequency' in opts:
				if '--docID' in opts:
					print(wordInDocFrequency(databaseName, opts['--frequency'], opts['--docID']))
				else:
					print(wordFrequency(databaseName, opts['--frequency']))
			elif '--findURL' in opts:
				print(findURL(databaseName, opts['--findURL']))
			elif '--findDocID' in opts:
				print(findDocID(databaseName, opts['--findDocID']))
			elif '--docFrequency' in opts:
				print(documentFrequency(databaseName, opts['--docFrequency']))

	except getopt.GetoptError as err:
		print(err) 
		sys.exit(2)