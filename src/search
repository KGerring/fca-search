#!/Library/Frameworks/Python.framework/Versions/3.2/bin/python3

from retrieval.search_engine import SearchEngine
from other.data import getStopWords
from other.constants import DATABASES_FOLDER, DATA_FOLDER, SETTINGS_FILE
from retrieval.index import Index
from fca_extension.fca_search_engine import FCASearchEngine
import getopt, sys
from common.io import readfile
from preprocess.index_manager import IndexManager
from other.stopwatch import Stopwatch
from other.settings import Settings
import json

## functions 
def printSearch(searchResults, stopwatch):
	results = searchResults['origin']
	documents = results['documents']
	
	if documents:
		maxURL = max(len(x['url']) for x in documents) - len('http://')
		maxKeywords = max([len(repr([(x[0], round(x[1])) for x in y['keywords'][:3]])) for y in documents])
		maxPos = len(str(len(documents)))
		maxScore = max(len(str(round(x['score'], 3))) for x in documents) + 1
		
		print(''.ljust(maxPos), 'URL'.ljust(maxURL), 'KEYWORDS'.ljust(maxKeywords), 'SCORE'.ljust(maxScore), 'WORDS')
		for pos, item in enumerate(documents):
			score = str(round(item['score'], 3))
			url = item['url'].replace('http://', '')
			keywords = repr([(x[0], round(x[1])) for x in item['keywords'][:3]])
			wordscount = item['words']
	
			print(str(pos+1).ljust(maxPos), url.ljust(maxURL), keywords.ljust(maxKeywords), score.ljust(maxScore), wordscount)
	
	gen, sib, spec, meta, sugg = getFcaExt(searchResults)
	if stopwatch:
		meta['time'] = str(round(stopwatch.total, 4))
	
	print()
	print('Generalization' + str(gen))
	print('Specialization:' + str(spec))
	print('Siblings:' + str(sib))
	print('Search query: ' + str(results['parsedQuery']))
	print('Number results: ' + str(len(documents)))
	print()
	print('Did you mean: {0}'.format(sugg))
	print('Meta: {0}'.format(meta))

	
def printJson(searchResults, stopwatch):
	gen, sib, spec, meta, spellcheck = getFcaExt(searchResults)
	
	results = searchResults['origin']
	documents = results['documents']
	fca = {'gen':gen, 'spec':spec, 'sib':sib}
	if stopwatch:
		meta['time'] = stopwatch.total
	data = {'documents':documents, 'fca': fca, 'meta' : meta, 'spellcheck':spellcheck}
	jsonData = json.dumps(data)
	print(jsonData)
	
def getFcaExt(searchResults):
	spec = searchResults['specialization']
	gen = searchResults['generalization']
	sib = searchResults['siblings']
	meta = searchResults['meta']
	spellcheck = searchResults['suggestions']
	
	return gen, sib, spec, meta, spellcheck

def search(databaseName, query, outFormat, stopwatch):
	database = DATABASES_FOLDER + databaseName + '/'
	settings = Settings(database + SETTINGS_FILE)
	index = Index(database, settings)
	searchEngine = SearchEngine(index, getStopWords())
	fca = FCASearchEngine(searchEngine, index, settings)
	searchResults = fca.search(query)
	if stopwatch:
		stopwatch.elapsed('Done')
	if outFormat == 'console':
		printSearch(searchResults, stopwatch)
	elif outFormat == 'json':
		printJson(searchResults, stopwatch)
	
def buildIndex(databaseName, linksSourcePath, maxLinksToDownload):
	settings = Settings(DATA_FOLDER + SETTINGS_FILE)
	database = DATABASES_FOLDER + databaseName + '/'
	links = readfile(linksSourcePath).splitlines()
	if maxLinksToDownload:
		links = links[:int(maxLinksToDownload)]
	indexManager = IndexManager(settings)
	indexManager.shutUp = False
	indexManager.build(links, database, getStopWords())
	
def rebuildIndex(databaseName, newLinks = []):
	database = DATABASES_FOLDER + databaseName + '/'
	settings = Settings(database + SETTINGS_FILE)
	indexManager = IndexManager(settings)
	indexManager.shutUp = False
	indexManager.rebuild(newLinks, database, getStopWords())
	
	
## main

if __name__ == '__main__':
	stopwatch = Stopwatch().start()
	
	try:
		opts, args = getopt.getopt(sys.argv[1:], 'd:q:b:p:f:m:r:a:t')
		opts = dict(opts)
		print(opts)
		
		# search in database
		if '-q' in opts:
			outFormat = opts.get('-f', 'console')
			search(opts['-d'], opts['-q'], outFormat, stopwatch if '-t' not in opts else None)
				
		# build index
		elif '-b' in opts:
			databaseName = opts['-b']
			linksSourcePath = opts.get('-p', DATA_FOLDER + opts['-b'] + '.txt')
			maxLinksToDownload = int(opts.get('-m', 0))
			buildIndex(databaseName, linksSourcePath, maxLinksToDownload)
		
		# refresh index
		elif '-r' in opts:
			rebuildIndex(opts['-r'])
			
		# add links to index
		elif '-a' in opts:
			databaseName = opts['-a']
			linksSourcePath = opts.get('-p')
			links = readfile(linksSourcePath).splitlines()
			rebuildIndex(databaseName, links)
	except getopt.GetoptError as err:
		print(err) 
		sys.exit(2)